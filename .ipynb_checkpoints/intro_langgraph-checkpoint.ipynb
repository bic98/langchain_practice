{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph ì™„ë²½ ì…ë¬¸ ê°€ì´ë“œ ğŸš€\n",
    "\n",
    "## ğŸ“š LangGraphë€ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "LangGraphëŠ” **ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°**ë¥¼ ì‚¬ìš©í•˜ì—¬ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“œëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ¤” ì™œ LangGraphë¥¼ ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”?\n",
    "\n",
    "1. **ëª…í™•í•œ íë¦„ ì œì–´**: ê° ë‹¨ê³„(ë…¸ë“œ)ë¥¼ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "2. **ìœ ì—°í•œ ë¼ìš°íŒ…**: ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ê²½ë¡œë¡œ íë¦„ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "3. **ìƒíƒœ ê´€ë¦¬**: ê° ë‹¨ê³„ë§ˆë‹¤ ìƒíƒœë¥¼ ì¶”ì í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "4. **ì‹œê°í™”**: ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì‰½ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "### ğŸ¯ í•µì‹¬ ê°œë…\n",
    "\n",
    "- **ë…¸ë“œ(Node)**: ê° ì‘ì—… ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜\n",
    "- **ì—£ì§€(Edge)**: ë…¸ë“œ ê°„ì˜ ì—°ê²°ê³¼ íë¦„\n",
    "- **ìƒíƒœ(State)**: ì›Œí¬í”Œë¡œìš° ì „ì²´ì—ì„œ ê³µìœ ë˜ëŠ” ë°ì´í„°\n",
    "- **ê·¸ë˜í”„(Graph)**: ë…¸ë“œì™€ ì—£ì§€ë¡œ êµ¬ì„±ëœ ì „ì²´ ì›Œí¬í”Œë¡œìš°\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ ì‹¤ìŠµ í”„ë¡œì íŠ¸: í…ìŠ¤íŠ¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **3ë‹¨ê³„ í…ìŠ¤íŠ¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸**ì„ ë§Œë“¤ì–´ë´…ë‹ˆë‹¤:\n",
    "\n",
    "```\n",
    "ì…ë ¥ í…ìŠ¤íŠ¸ â†’ [1ë‹¨ê³„: ë¶„ë¥˜] â†’ [2ë‹¨ê³„: ê°œì²´ëª… ì¶”ì¶œ] â†’ [3ë‹¨ê³„: ìš”ì•½] â†’ ê²°ê³¼\n",
    "```\n",
    "\n",
    "1. **í…ìŠ¤íŠ¸ ë¶„ë¥˜**: ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì—°êµ¬ë…¼ë¬¸, ê¸°íƒ€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜\n",
    "2. **ê°œì²´ëª… ì¶”ì¶œ**: ì¸ëª…, ê¸°ê´€ëª…, ì§€ëª… ë“± ì£¼ìš” ê°œì²´ ì¶”ì¶œ\n",
    "3. **í…ìŠ¤íŠ¸ ìš”ì•½**: í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1ë‹¨ê³„: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "### ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì—­í•  ì„¤ëª…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ====================\n",
    "import os  # ìš´ì˜ì²´ì œì™€ ìƒí˜¸ì‘ìš© (í™˜ê²½ë³€ìˆ˜, íŒŒì¼ì‹œìŠ¤í…œ ë“±)\n",
    "from typing import TypedDict, List  # íƒ€ì… íŒíŒ…ìœ¼ë¡œ ì½”ë“œì˜ ì•ˆì •ì„±ê³¼ ê°€ë…ì„± í–¥ìƒ\n",
    "\n",
    "# ==================== LangGraph í•µì‹¬ ====================\n",
    "# StateGraph: ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ê·¸ë˜í”„ë¥¼ ë§Œë“œëŠ” í•µì‹¬ í´ë˜ìŠ¤\n",
    "#   - ê° ë…¸ë“œëŠ” ìƒíƒœë¥¼ ì½ê³  ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "#   - ì „ì²´ ì›Œí¬í”Œë¡œìš°ì˜ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤\n",
    "from langgraph.graph import StateGraph, END\n",
    "# END: ê·¸ë˜í”„ì˜ ì¢…ë£Œ ì§€ì ì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ ë…¸ë“œ\n",
    "\n",
    "# ==================== LangChain ì»´í¬ë„ŒíŠ¸ ====================\n",
    "# PromptTemplate: AIì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ë¥¼ í…œí”Œë¦¿í™”\n",
    "#   - ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "#   - ì¼ê´€ëœ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ChatOpenAI: OpenAIì˜ GPT ëª¨ë¸ê³¼ í†µì‹ í•˜ëŠ” í´ë˜ìŠ¤\n",
    "#   - API í˜¸ì¶œì„ ê°„í¸í•˜ê²Œ ì²˜ë¦¬\n",
    "#   - ì‘ë‹µì„ í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ë°›ìŒ\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# HumanMessage: ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤\n",
    "#   - AIì™€ì˜ ëŒ€í™”ì—ì„œ ì‚¬ëŒì˜ ë©”ì‹œì§€ë¥¼ êµ¬ì¡°í™”\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# ==================== ì‹œê°í™” ë„êµ¬ ====================\n",
    "# MermaidDrawMethod: Mermaid ë¬¸ë²•ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”\n",
    "#   - ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ í‘œí˜„\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "# ==================== Jupyter ë””ìŠ¤í”Œë ˆì´ ====================\n",
    "# display, Image: ë…¸íŠ¸ë¶ì— ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# ==================== í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ ====================\n",
    "# load_dotenv: .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œ\n",
    "#   - API í‚¤ ê°™ì€ ë¯¼ê°í•œ ì •ë³´ë¥¼ ì½”ë“œì—ì„œ ë¶„ë¦¬\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ì™œ ì´ëŸ° êµ¬ì¡°ë¡œ ì„í¬íŠ¸í• ê¹Œ?\n",
    "\n",
    "1. **ëª¨ë“ˆí™”**: ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” íŠ¹ì • ì—­í• ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤\n",
    "2. **ì¬ì‚¬ìš©ì„±**: í•„ìš”í•œ ê¸°ëŠ¥ë§Œ ì„ íƒì ìœ¼ë¡œ ì„í¬íŠ¸í•©ë‹ˆë‹¤\n",
    "3. **ìœ ì§€ë³´ìˆ˜**: ì½”ë“œë¥¼ ë…¼ë¦¬ì  ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬ê°€ ì‰½ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”‘ 2ë‹¨ê³„: API í‚¤ ì„¤ì •\n",
    "\n",
    "### í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ \n",
    "\n",
    "1. **ë³´ì•ˆ**: API í‚¤ë¥¼ ì½”ë“œì— ì§ì ‘ ë…¸ì¶œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n",
    "2. **ê´€ë¦¬**: ì—¬ëŸ¬ í”„ë¡œì íŠ¸ì—ì„œ ê°™ì€ í‚¤ë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "3. **ê³µìœ **: ì½”ë“œë¥¼ ê³µìœ í•  ë•Œ í‚¤ë¥¼ ì œì™¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
    "# .env íŒŒì¼ ì˜ˆì‹œ:\n",
    "# OPENAI_API_KEY=sk-your-api-key-here\n",
    "load_dotenv()\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì™€ì„œ ì„¤ì •\n",
    "# os.environì€ ì‹œìŠ¤í…œì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# API í‚¤ê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë³´ì•ˆì„ ìœ„í•´ ì¼ë¶€ë§Œ í‘œì‹œ)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(f\"âœ… API í‚¤ ë¡œë“œ ì„±ê³µ! (ì• 10ì: {api_key[:10]}...)\")\n",
    "else:\n",
    "    print(\"âŒ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ 3ë‹¨ê³„: ìƒíƒœ(State) ì •ì˜ì™€ LLM ì´ˆê¸°í™”\n",
    "\n",
    "### ìƒíƒœ(State)ë€?\n",
    "\n",
    "ìƒíƒœëŠ” ì›Œí¬í”Œë¡œìš° ì „ì²´ì—ì„œ ê³µìœ ë˜ëŠ” **ë°ì´í„° ì €ì¥ì†Œ**ì…ë‹ˆë‹¤.\n",
    "ê° ë…¸ë“œëŠ” ìƒíƒœë¥¼ ì½ê³ , ìˆ˜ì •í•˜ê³ , ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "### TypedDictë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ \n",
    "\n",
    "1. **íƒ€ì… ì•ˆì •ì„±**: ê° í•„ë“œì˜ íƒ€ì…ì„ ëª…ì‹œí•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤\n",
    "2. **ìë™ì™„ì„±**: IDEì—ì„œ ìë™ì™„ì„± ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "3. **ë¬¸ì„œí™”**: ì½”ë“œë§Œìœ¼ë¡œë„ ë°ì´í„° êµ¬ì¡°ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ìƒíƒœ í´ë˜ìŠ¤ ì •ì˜ ====================\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "    \n",
    "    ê° í•„ë“œì˜ ì˜ë¯¸:\n",
    "    - text: ë¶„ì„í•  ì›ë³¸ í…ìŠ¤íŠ¸ (ì…ë ¥)\n",
    "    - classification: í…ìŠ¤íŠ¸ì˜ ë¶„ë¥˜ ê²°ê³¼ (ë‰´ìŠ¤/ë¸”ë¡œê·¸/ì—°êµ¬/ê¸°íƒ€)\n",
    "    - entities: ì¶”ì¶œëœ ê°œì²´ëª… ë¦¬ìŠ¤íŠ¸ (ì¸ëª…, ê¸°ê´€ëª…, ì§€ëª… ë“±)\n",
    "    - summary: í…ìŠ¤íŠ¸ì˜ ìš”ì•½ (í•œ ë¬¸ì¥)\n",
    "    \"\"\"\n",
    "    text: str              # ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "    classification: str    # ë¶„ë¥˜ ê²°ê³¼\n",
    "    entities: List[str]    # ê°œì²´ëª… ë¦¬ìŠ¤íŠ¸\n",
    "    summary: str           # ìš”ì•½ë¬¸\n",
    "\n",
    "# ==================== LLM ì´ˆê¸°í™” ====================\n",
    "# ChatOpenAI: OpenAIì˜ GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # ëª¨ë¸ ì„ íƒ: gpt-4o-miniëŠ” ë¹ ë¥´ê³  ë¹„ìš© íš¨ìœ¨ì \n",
    "    temperature=0         # temperature=0: ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹µë³€ë§Œ ìƒì„± (ì¼ê´€ì„± â†‘)\n",
    "                          # temperature=1: ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ ë‹µë³€ ìƒì„± (ë¬´ì‘ìœ„ì„± â†‘)\n",
    ")\n",
    "\n",
    "print(\"âœ… ìƒíƒœ í´ë˜ìŠ¤ì™€ LLM ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
    "print(f\"   ì‚¬ìš© ëª¨ë¸: {llm.model_name}\")\n",
    "print(f\"   Temperature: {llm.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Temperature ë§¤ê°œë³€ìˆ˜ ì´í•´í•˜ê¸°\n",
    "\n",
    "TemperatureëŠ” AIì˜ **ì°½ì˜ì„± vs ì¼ê´€ì„±**ì„ ì¡°ì ˆí•©ë‹ˆë‹¤:\n",
    "\n",
    "| Temperature | íŠ¹ì§• | ì‚¬ìš© ì˜ˆì‹œ |\n",
    "|-------------|------|----------|\n",
    "| 0.0 | ê°€ì¥ í™•ë¥  ë†’ì€ ë‹µë³€ë§Œ ì„ íƒ (ê²°ì •ë¡ ì ) | ë¶„ë¥˜, ë²ˆì—­, ìš”ì•½ |\n",
    "| 0.5 | ì ë‹¹í•œ ì°½ì˜ì„± | ì¼ë°˜ì ì¸ ëŒ€í™” |\n",
    "| 1.0 | ë§¤ìš° ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ ë‹µë³€ | ìŠ¤í† ë¦¬ ìƒì„±, ë¸Œë ˆì¸ìŠ¤í† ë° |\n",
    "\n",
    "ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” **ì¼ê´€ëœ ê²°ê³¼**ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ temperature=0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 4ë‹¨ê³„: ë…¸ë“œ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "### ë…¸ë“œ(Node)ë€?\n",
    "\n",
    "ë…¸ë“œëŠ” ì›Œí¬í”Œë¡œìš°ì˜ **ê° ì‘ì—… ë‹¨ê³„**ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "- ì…ë ¥: í˜„ì¬ ìƒíƒœ(State)\n",
    "- ì²˜ë¦¬: íŠ¹ì • ì‘ì—… ìˆ˜í–‰\n",
    "- ì¶œë ¥: ì—…ë°ì´íŠ¸ëœ ìƒíƒœì˜ ì¼ë¶€\n",
    "\n",
    "### 3ê°œì˜ ë…¸ë“œ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "classification_node     â†’ í…ìŠ¤íŠ¸ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜\n",
    "       â†“\n",
    "entity_extraction_node  â†’ ì¤‘ìš”í•œ ê°œì²´ëª… ì¶”ì¶œ\n",
    "       â†“\n",
    "summarization_node      â†’ í•µì‹¬ ë‚´ìš© ìš”ì•½\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ë…¸ë“œ 1: í…ìŠ¤íŠ¸ ë¶„ë¥˜ ====================\n",
    "def classification_node(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ë¥¼ 4ê°€ì§€ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì™œ ì´ë ‡ê²Œ ë§Œë“¤ê¹Œ?\n",
    "    1. PromptTemplate: í”„ë¡¬í”„íŠ¸ë¥¼ í…œí”Œë¦¿í™”í•˜ì—¬ ì¬ì‚¬ìš©ì„±ì„ ë†’ì…ë‹ˆë‹¤\n",
    "    2. HumanMessage: LangChainì˜ í‘œì¤€ ë©”ì‹œì§€ í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "    3. strip(): ì‘ë‹µì˜ ì•ë’¤ ê³µë°±ì„ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤\n",
    "    \n",
    "    Args:\n",
    "        state: í˜„ì¬ ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœ\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"classification\": \"News\"} í˜•íƒœë¡œ ë¶„ë¥˜ ê²°ê³¼ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ” [ë…¸ë“œ 1] í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹œì‘...\")\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "    # {text}ëŠ” ì‹¤í–‰ ì‹œ ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ì¹˜í™˜ë©ë‹ˆë‹¤\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],  # í…œí”Œë¦¿ì— ì‚¬ìš©í•  ë³€ìˆ˜ ëª©ë¡\n",
    "        template=\"\"\"\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:\n",
    "- News (ë‰´ìŠ¤)\n",
    "- Blog (ë¸”ë¡œê·¸)\n",
    "- Research (ì—°êµ¬ë…¼ë¬¸)\n",
    "- Other (ê¸°íƒ€)\n",
    "\n",
    "í…ìŠ¤íŠ¸:\n",
    "{text}\n",
    "\n",
    "ë¶„ë¥˜ ê²°ê³¼ (í•˜ë‚˜ë§Œ ë‹µë³€):\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ì— ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì„œ ë©”ì‹œì§€ ìƒì„±\n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    \n",
    "    # LLMì— ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  ì‘ë‹µ ë°›ê¸°\n",
    "    classification = llm.invoke([message]).content.strip()\n",
    "    \n",
    "    print(f\"   âœ… ë¶„ë¥˜ ì™„ë£Œ: {classification}\")\n",
    "    \n",
    "    # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜ (ìƒíƒœì˜ classification í•„ë“œê°€ ì—…ë°ì´íŠ¸ë¨)\n",
    "    return {\"classification\": classification}\n",
    "\n",
    "\n",
    "# ==================== ë…¸ë“œ 2: ê°œì²´ëª… ì¶”ì¶œ ====================\n",
    "def entity_extraction_node(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ ê°œì²´ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ê°œì²´ëª…(Named Entity)ì´ë€?\n",
    "    - ì¸ëª…: í™ê¸¸ë™, Steve Jobs, ê¹€ì² ìˆ˜\n",
    "    - ê¸°ê´€ëª…: ì‚¼ì„±ì „ì, Apple, ì„œìš¸ëŒ€í•™êµ\n",
    "    - ì§€ëª…: ì„œìš¸, San Francisco, í•œê°•\n",
    "    \n",
    "    ì™œ split(\", \")ë¥¼ ì‚¬ìš©í• ê¹Œ?\n",
    "    - LLMì´ \"Apple, Google, Microsoft\" í˜•íƒœë¡œ ì‘ë‹µí•˜ë¯€ë¡œ\n",
    "    - ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    \n",
    "    Args:\n",
    "        state: í˜„ì¬ ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœ\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"entities\": [\"ê°œì²´1\", \"ê°œì²´2\", ...]} í˜•íƒœë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ¢ [ë…¸ë“œ 2] ê°œì²´ëª… ì¶”ì¶œ ì‹œì‘...\")\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ê°œì²´ëª…ì„ ì¶”ì¶œí•˜ì„¸ìš”:\n",
    "- ì¸ëª… (ì‚¬ëŒ ì´ë¦„)\n",
    "- ê¸°ê´€ëª… (íšŒì‚¬, ì¡°ì§, í•™êµ ë“±)\n",
    "- ì§€ëª… (ë„ì‹œ, êµ­ê°€, ì¥ì†Œ ë“±)\n",
    "\n",
    "ê²°ê³¼ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.\n",
    "ì˜ˆì‹œ: Apple, Steve Jobs, Cupertino\n",
    "\n",
    "í…ìŠ¤íŠ¸:\n",
    "{text}\n",
    "\n",
    "ê°œì²´ëª…:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    \n",
    "    # LLM ì‘ë‹µì„ ë°›ì•„ì„œ ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    entities_str = llm.invoke([message]).content.strip()\n",
    "    entities = [e.strip() for e in entities_str.split(\",\")]  # ê° í•­ëª©ì˜ ê³µë°±ë„ ì œê±°\n",
    "    \n",
    "    print(f\"   âœ… ì¶”ì¶œ ì™„ë£Œ: {len(entities)}ê°œ ê°œì²´ëª… ë°œê²¬\")\n",
    "    for i, entity in enumerate(entities, 1):\n",
    "        print(f\"      {i}. {entity}\")\n",
    "    \n",
    "    return {\"entities\": entities}\n",
    "\n",
    "\n",
    "# ==================== ë…¸ë“œ 3: í…ìŠ¤íŠ¸ ìš”ì•½ ====================\n",
    "def summarization_node(state: State) -> dict:\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    ì™œ \"í•œ ë¬¸ì¥\"ìœ¼ë¡œ ì œí•œí• ê¹Œ?\n",
    "    1. ëª…í™•ì„±: í•µì‹¬ë§Œ ì „ë‹¬í•©ë‹ˆë‹¤\n",
    "    2. ì¼ê´€ì„±: ê¸¸ì´ê°€ ì˜ˆì¸¡ ê°€ëŠ¥í•©ë‹ˆë‹¤\n",
    "    3. íš¨ìœ¨ì„±: ë¹ ë¥´ê²Œ ë‚´ìš©ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "    \n",
    "    Args:\n",
    "        state: í˜„ì¬ ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœ\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\"summary\": \"ìš”ì•½ë¬¸\"} í˜•íƒœë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“ [ë…¸ë“œ 3] í…ìŠ¤íŠ¸ ìš”ì•½ ì‹œì‘...\")\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"\"\"\n",
    "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ **í•œ ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n",
    "í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ì„¸ìš”.\n",
    "\n",
    "í…ìŠ¤íŠ¸:\n",
    "{text}\n",
    "\n",
    "ìš”ì•½:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    summary = llm.invoke([message]).content.strip()\n",
    "    \n",
    "    print(f\"   âœ… ìš”ì•½ ì™„ë£Œ\")\n",
    "    print(f\"   ğŸ“„ {summary}\")\n",
    "    \n",
    "    return {\"summary\": summary}\n",
    "\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"   - classification_node: í…ìŠ¤íŠ¸ ë¶„ë¥˜\")\n",
    "print(\"   - entity_extraction_node: ê°œì²´ëª… ì¶”ì¶œ\")\n",
    "print(\"   - summarization_node: í…ìŠ¤íŠ¸ ìš”ì•½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ ë…¸ë“œ ì„¤ê³„ ì›ì¹™\n",
    "\n",
    "1. **ë‹¨ì¼ ì±…ì„**: ê° ë…¸ë“œëŠ” í•˜ë‚˜ì˜ ì‘ì—…ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤\n",
    "2. **ìƒíƒœ ì˜ì¡´ì„±**: ë…¸ë“œëŠ” ìƒíƒœë¥¼ ì½ê³  í•„ìš”í•œ ë¶€ë¶„ë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤\n",
    "3. **ë…ë¦½ì„±**: ê° ë…¸ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "4. **ì¬ì‚¬ìš©ì„±**: ë‹¤ë¥¸ ì›Œí¬í”Œë¡œìš°ì—ì„œë„ ê°™ì€ ë…¸ë“œë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ 5ë‹¨ê³„: ì›Œí¬í”Œë¡œìš° êµ¬ì¶•\n",
    "\n",
    "### ê·¸ë˜í”„ êµ¬ì¶• ê³¼ì •\n",
    "\n",
    "```\n",
    "1. StateGraph ìƒì„± â†’ ìƒíƒœ êµ¬ì¡° ì •ì˜\n",
    "2. ë…¸ë“œ ì¶”ê°€ â†’ ê° ì‘ì—… ë‹¨ê³„ ë“±ë¡\n",
    "3. ì—£ì§€ ì¶”ê°€ â†’ ë…¸ë“œ ê°„ ì—°ê²°\n",
    "4. ì»´íŒŒì¼ â†’ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•±ìœ¼ë¡œ ë³€í™˜\n",
    "```\n",
    "\n",
    "### ì™œ ì´ëŸ° ìˆœì„œë¡œ êµ¬ì¶•í• ê¹Œ?\n",
    "\n",
    "1. **ëª…í™•í•œ êµ¬ì¡°**: ìƒíƒœ â†’ ë…¸ë“œ â†’ ì—°ê²° ìˆœì„œë¡œ ì ì§„ì ìœ¼ë¡œ êµ¬ì¶•\n",
    "2. **ìœ ì—°ì„±**: ë‚˜ì¤‘ì— ë…¸ë“œë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì—°ê²°ì„ ë³€ê²½í•˜ê¸° ì‰¬ì›€\n",
    "3. **ë””ë²„ê¹…**: ê° ë‹¨ê³„ë¥¼ í™•ì¸í•˜ë©´ì„œ êµ¬ì¶•í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ—ï¸ ì›Œí¬í”Œë¡œìš° êµ¬ì¶• ì‹œì‘...\\n\")\n",
    "\n",
    "# ==================== 1. StateGraph ìƒì„± ====================\n",
    "# StateGraph: ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ê·¸ë˜í”„ ê°ì²´\n",
    "# State í´ë˜ìŠ¤ë¥¼ ì „ë‹¬í•˜ì—¬ ì–´ë–¤ ë°ì´í„°ë¥¼ ì¶”ì í• ì§€ ì •ì˜\n",
    "workflow = StateGraph(State)\n",
    "print(\"âœ… 1ë‹¨ê³„: StateGraph ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "# ==================== 2. ë…¸ë“œ ì¶”ê°€ ====================\n",
    "# add_node(\"ë…¸ë“œì´ë¦„\", ì‹¤í–‰í• _í•¨ìˆ˜)\n",
    "# ë…¸ë“œ ì´ë¦„ì€ ì›Œí¬í”Œë¡œìš°ì—ì„œ í•´ë‹¹ ë‹¨ê³„ë¥¼ ì‹ë³„í•˜ëŠ” IDì…ë‹ˆë‹¤\n",
    "workflow.add_node(\"classification_node\", classification_node)\n",
    "print(\"âœ… 2-1ë‹¨ê³„: classification_node ì¶”ê°€\")\n",
    "\n",
    "workflow.add_node(\"entity_extraction\", entity_extraction_node)\n",
    "print(\"âœ… 2-2ë‹¨ê³„: entity_extraction ì¶”ê°€\")\n",
    "\n",
    "workflow.add_node(\"summarization\", summarization_node)\n",
    "print(\"âœ… 2-3ë‹¨ê³„: summarization ì¶”ê°€\")\n",
    "\n",
    "# ==================== 3. ì—£ì§€(íë¦„) ì¶”ê°€ ====================\n",
    "# set_entry_point: ì›Œí¬í”Œë¡œìš°ì˜ ì‹œì‘ì  ì„¤ì •\n",
    "workflow.set_entry_point(\"classification_node\")\n",
    "print(\"\\nâœ… 3-1ë‹¨ê³„: ì‹œì‘ì  ì„¤ì • (classification_node)\")\n",
    "\n",
    "# add_edge: ë…¸ë“œ ê°„ ì—°ê²° (A â†’ B)\n",
    "# \"classification_nodeê°€ ëë‚˜ë©´ entity_extractionìœ¼ë¡œ ì´ë™\"\n",
    "workflow.add_edge(\"classification_node\", \"entity_extraction\")\n",
    "print(\"âœ… 3-2ë‹¨ê³„: classification_node â†’ entity_extraction ì—°ê²°\")\n",
    "\n",
    "workflow.add_edge(\"entity_extraction\", \"summarization\")\n",
    "print(\"âœ… 3-3ë‹¨ê³„: entity_extraction â†’ summarization ì—°ê²°\")\n",
    "\n",
    "# END: íŠ¹ìˆ˜ ë…¸ë“œë¡œ ì›Œí¬í”Œë¡œìš°ì˜ ì¢…ë£Œë¥¼ ë‚˜íƒ€ëƒ„\n",
    "workflow.add_edge(\"summarization\", END)\n",
    "print(\"âœ… 3-4ë‹¨ê³„: summarization â†’ END ì—°ê²°\")\n",
    "\n",
    "# ==================== 4. ì»´íŒŒì¼ ====================\n",
    "# compile(): ê·¸ë˜í”„ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•±ìœ¼ë¡œ ë³€í™˜\n",
    "# ì´ ì‹œì ì—ì„œ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ê²€ì¦í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤\n",
    "app = workflow.compile()\n",
    "print(\"\\nâœ… 4ë‹¨ê³„: ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ‰ ì›Œí¬í”Œë¡œìš° êµ¬ì¶• ì™„ë£Œ!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nğŸ“Š ì›Œí¬í”Œë¡œìš° êµ¬ì¡°:\")\n",
    "print(\"   START\")\n",
    "print(\"     â†“\")\n",
    "print(\"   [1. í…ìŠ¤íŠ¸ ë¶„ë¥˜]\")\n",
    "print(\"     â†“\")\n",
    "print(\"   [2. ê°œì²´ëª… ì¶”ì¶œ]\")\n",
    "print(\"     â†“\")\n",
    "print(\"   [3. í…ìŠ¤íŠ¸ ìš”ì•½]\")\n",
    "print(\"     â†“\")\n",
    "print(\"   END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ì½”ë“œ ë¶„ì„: add_edge vs set_entry_point\n",
    "\n",
    "| ë©”ì„œë“œ | ìš©ë„ | ì˜ˆì‹œ |\n",
    "|--------|------|------|\n",
    "| `set_entry_point(\"A\")` | ì›Œí¬í”Œë¡œìš°ì˜ ì‹œì‘ ë…¸ë“œ ì§€ì • | ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì„ ì²« ë²ˆì§¸ ë…¸ë“œ |\n",
    "| `add_edge(\"A\", \"B\")` | A ë…¸ë“œ ì‹¤í–‰ í›„ B ë…¸ë“œë¡œ ì´ë™ | ìˆœì°¨ì  ì²˜ë¦¬ íë¦„ |\n",
    "| `add_edge(\"A\", END)` | A ë…¸ë“œ ì‹¤í–‰ í›„ ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ | ë§ˆì§€ë§‰ ë…¸ë“œ ì§€ì • |\n",
    "\n",
    "### ğŸ’¡ ì™œ compile()ì´ í•„ìš”í• ê¹Œ?\n",
    "\n",
    "1. **ê²€ì¦**: ê·¸ë˜í”„ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸ (ëŠê¸´ ê²½ë¡œ, ìˆœí™˜ ì°¸ì¡° ë“±)\n",
    "2. **ìµœì í™”**: ì‹¤í–‰ ê²½ë¡œë¥¼ ìµœì í™”í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ\n",
    "3. **ê³ ì •**: ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ê³ ì •í•˜ì—¬ ì‹¤í–‰ ì¤‘ ë³€ê²½ ë°©ì§€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ 6ë‹¨ê³„: ì›Œí¬í”Œë¡œìš° ì‹œê°í™”\n",
    "\n",
    "### Mermaidë€?\n",
    "\n",
    "MermaidëŠ” í…ìŠ¤íŠ¸ë¡œ ë‹¤ì´ì–´ê·¸ë¨ì„ ê·¸ë¦¬ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "- ê°„ë‹¨í•œ ë¬¸ë²•ìœ¼ë¡œ ë³µì¡í•œ ê·¸ë˜í”„ í‘œí˜„\n",
    "- GitHub, Notion ë“±ì—ì„œ ì§€ì›\n",
    "- LangGraphì™€ ì™„ë²½ í˜¸í™˜\n",
    "\n",
    "### ì‹œê°í™”ì˜ ì¥ì \n",
    "\n",
    "1. **ì´í•´**: ì›Œí¬í”Œë¡œìš°ë¥¼ í•œëˆˆì— íŒŒì•…\n",
    "2. **ì†Œí†µ**: íŒ€ì›ì—ê²Œ êµ¬ì¡°ë¥¼ ì‰½ê²Œ ì„¤ëª…\n",
    "3. **ë””ë²„ê¹…**: íë¦„ ë¬¸ì œë¥¼ ë¹ ë¥´ê²Œ ë°œê²¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¨ ì›Œí¬í”Œë¡œìš° ì‹œê°í™” ì¤‘...\\n\")\n",
    "\n",
    "try:\n",
    "    # get_graph(): ì»´íŒŒì¼ëœ ì•±ì—ì„œ ê·¸ë˜í”„ êµ¬ì¡° ì¶”ì¶œ\n",
    "    # draw_mermaid_png(): Mermaid ë¬¸ë²•ìœ¼ë¡œ PNG ì´ë¯¸ì§€ ìƒì„±\n",
    "    # MermaidDrawMethod.API: Mermaid APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë Œë”ë§\n",
    "    display(\n",
    "        Image(\n",
    "            app.get_graph().draw_mermaid_png(\n",
    "                draw_method=MermaidDrawMethod.API,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print(\"âœ… ì‹œê°í™” ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ì‹œê°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"   (ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ Mermaid API ì ‘ê·¼ì„ í™•ì¸í•˜ì„¸ìš”)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Mermaid ë¬¸ë²• ê°„ë‹¨ ì„¤ëª…\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    START --> classification_node\n",
    "    classification_node --> entity_extraction\n",
    "    entity_extraction --> summarization\n",
    "    summarization --> END\n",
    "```\n",
    "\n",
    "- `graph TD`: Top-Down ë°©í–¥ ê·¸ë˜í”„\n",
    "- `-->`: í™”ì‚´í‘œ (íë¦„ ë°©í–¥)\n",
    "- ê° ì¤„ì€ í•˜ë‚˜ì˜ ì—°ê²°ì„ ë‚˜íƒ€ëƒ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 7ë‹¨ê³„: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### invoke() ë©”ì„œë“œ\n",
    "\n",
    "`app.invoke(state_input)`ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.\n",
    "\n",
    "**ì‹¤í–‰ ê³¼ì •:**\n",
    "1. ì…ë ¥ ìƒíƒœë¥¼ ë°›ìŠµë‹ˆë‹¤\n",
    "2. ì‹œì‘ì (entry point)ë¶€í„° ë…¸ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤\n",
    "3. ê° ë…¸ë“œê°€ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤\n",
    "4. END ë…¸ë“œì— ë„ë‹¬í•˜ë©´ ìµœì¢… ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤\n",
    "\n",
    "### ì˜ˆì œ í…ìŠ¤íŠ¸ ì„ íƒ\n",
    "\n",
    "OpenAIì˜ GPT-4 ë°œí‘œ ê´€ë ¨ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ëª…í™•í•œ ì£¼ì œ\n",
    "- ì—¬ëŸ¬ ê°œì²´ëª… í¬í•¨\n",
    "- ìš”ì•½í•˜ê¸° ì ì ˆí•œ ê¸¸ì´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==================== í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ====================\n",
    "sample_text = \"\"\"\n",
    "OpenAIëŠ” GPT-4 ëª¨ë¸ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ì „ë¬¸ ë²¤ì¹˜ë§ˆí¬ì—ì„œ \n",
    "ì¸ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì…ë‹ˆë‹¤. GPT-4ëŠ” AI ì‹œìŠ¤í…œì˜ \n",
    "ì •ë ¬ê³¼ ì•ˆì „ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ì´ì „ ë²„ì „ì¸ GPT-3ë³´ë‹¤ \n",
    "ë” íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. GPT-4 ëª¨ë¸ì€ ì•ìœ¼ë¡œ ëª‡ ë‹¬ ë‚´ì— \n",
    "ì¶œì‹œë  ì˜ˆì •ì´ë©°, ì—°êµ¬ ë° ê°œë°œ ëª©ì ìœ¼ë¡œ ëŒ€ì¤‘ì—ê²Œ ê³µê°œë  ê²ƒì…ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# ì…ë ¥ ìƒíƒœ ìƒì„±\n",
    "# text í•„ë“œë§Œ ì±„ìš°ê³ , ë‚˜ë¨¸ì§€ëŠ” ê° ë…¸ë“œê°€ ì±„ì›ë‹ˆë‹¤\n",
    "state_input = {\"text\": sample_text}\n",
    "\n",
    "print(\"ğŸ“„ ì…ë ¥ í…ìŠ¤íŠ¸:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ==================== ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ====================\n",
    "print(\"\\nâš™ï¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...\\n\")\n",
    "\n",
    "# invoke()ë¡œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n",
    "# ë‚´ë¶€ì ìœ¼ë¡œ ê° ë…¸ë“œê°€ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤\n",
    "result = app.invoke(state_input)\n",
    "\n",
    "# ==================== ê²°ê³¼ ì¶œë ¥ ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“Š ìµœì¢… ê²°ê³¼:\\n\")\n",
    "\n",
    "print(\"1ï¸âƒ£ í…ìŠ¤íŠ¸ ë¶„ë¥˜:\")\n",
    "print(f\"   {result['classification']}\")\n",
    "print()\n",
    "\n",
    "print(\"2ï¸âƒ£ ì¶”ì¶œëœ ê°œì²´ëª…:\")\n",
    "for i, entity in enumerate(result['entities'], 1):\n",
    "    print(f\"   {i}. {entity}\")\n",
    "print()\n",
    "\n",
    "print(\"3ï¸âƒ£ ìš”ì•½:\")\n",
    "print(f\"   {result['summary']}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ê²°ê³¼ ë¶„ì„\n",
    "\n",
    "**ì˜ˆìƒ ê²°ê³¼:**\n",
    "- **ë¶„ë¥˜**: News (ë‰´ìŠ¤ í˜•ì‹ì˜ ë°œí‘œë¬¸)\n",
    "- **ê°œì²´ëª…**: OpenAI, GPT-4, GPT-3 ë“±\n",
    "- **ìš”ì•½**: GPT-4ì˜ ì£¼ìš” íŠ¹ì§•ê³¼ ì¶œì‹œ ê³„íš\n",
    "\n",
    "### ğŸ’¡ ì‹¤í–‰ íë¦„ ì´í•´í•˜ê¸°\n",
    "\n",
    "```python\n",
    "state_input = {\"text\": \"...\"}  # ì´ˆê¸° ìƒíƒœ\n",
    "    â†“\n",
    "classification_node(state)  # {\"classification\": \"News\"} ì¶”ê°€\n",
    "    â†“\n",
    "entity_extraction_node(state)  # {\"entities\": [...]} ì¶”ê°€\n",
    "    â†“\n",
    "summarization_node(state)  # {\"summary\": \"...\"} ì¶”ê°€\n",
    "    â†“\n",
    "result = {  # ìµœì¢… ìƒíƒœ\n",
    "    \"text\": \"...\",\n",
    "    \"classification\": \"News\",\n",
    "    \"entities\": [...],\n",
    "    \"summary\": \"...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª 8ë‹¨ê³„: ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "### ì§ì ‘ í…ŒìŠ¤íŠ¸í•´ë³´ê¸°\n",
    "\n",
    "ì•„ë˜ ì…€ì—ì„œ `your_text` ë³€ìˆ˜ì— ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë„£ê³  ì‹¤í–‰í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ì—¬ê¸°ì— ì—¬ëŸ¬ë¶„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”! ====================\n",
    "your_text = \"\"\"\n",
    "ì—¬ê¸°ì— ë¶„ì„í•˜ê³  ì‹¶ì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
    "ì˜ˆ: ë‰´ìŠ¤ ê¸°ì‚¬, ë¸”ë¡œê·¸ ê¸€, ì—°êµ¬ ë…¼ë¬¸ ì´ˆë¡ ë“±\n",
    "\"\"\"\n",
    "\n",
    "# ==================== ì‹¤í–‰ ====================\n",
    "print(\"ğŸ”¬ ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "custom_state = {\"text\": your_text}\n",
    "custom_result = app.invoke(custom_state)\n",
    "\n",
    "print(\"ğŸ“Š ë¶„ì„ ê²°ê³¼:\\n\")\n",
    "print(f\"ë¶„ë¥˜: {custom_result['classification']}\")\n",
    "print(f\"ê°œì²´ëª…: {', '.join(custom_result['entities'])}\")\n",
    "print(f\"ìš”ì•½: {custom_result['summary']}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š 9ë‹¨ê³„: ì‹¬í™” í•™ìŠµ - ìƒíƒœ ì¶”ì \n",
    "\n",
    "### ê° ë…¸ë“œì˜ ì¤‘ê°„ ê²°ê³¼ í™•ì¸í•˜ê¸°\n",
    "\n",
    "ì›Œí¬í”Œë¡œìš°ì˜ ê° ë‹¨ê³„ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ìƒíƒœ ë³€í™”ë¥¼ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” ìƒíƒœ ë³€í™” ì¶”ì  ì‹¤ìŠµ\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ì´ˆê¸° ìƒíƒœ\n",
    "state = {\"text\": sample_text}\n",
    "print(\"0ï¸âƒ£ ì´ˆê¸° ìƒíƒœ:\")\n",
    "print(f\"   text: {state['text'][:50]}...\")\n",
    "print()\n",
    "\n",
    "# ë…¸ë“œ 1 ì‹¤í–‰\n",
    "result1 = classification_node(state)\n",
    "state.update(result1)  # ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "print(\"\\n1ï¸âƒ£ classification_node ì‹¤í–‰ í›„:\")\n",
    "print(f\"   classification: {state.get('classification', 'N/A')}\")\n",
    "print()\n",
    "\n",
    "# ë…¸ë“œ 2 ì‹¤í–‰\n",
    "result2 = entity_extraction_node(state)\n",
    "state.update(result2)\n",
    "print(\"\\n2ï¸âƒ£ entity_extraction_node ì‹¤í–‰ í›„:\")\n",
    "print(f\"   entities: {state.get('entities', [])}\")\n",
    "print()\n",
    "\n",
    "# ë…¸ë“œ 3 ì‹¤í–‰\n",
    "result3 = summarization_node(state)\n",
    "state.update(result3)\n",
    "print(\"\\n3ï¸âƒ£ summarization_node ì‹¤í–‰ í›„:\")\n",
    "print(f\"   summary: {state.get('summary', 'N/A')}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… ëª¨ë“  ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ!\")\n",
    "print(\"\\nğŸ’¡ Tip: ê° ë…¸ë“œê°€ ìƒíƒœì˜ íŠ¹ì • í•„ë“œë§Œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 10ë‹¨ê³„: í•µì‹¬ ê°œë… ì •ë¦¬\n",
    "\n",
    "### LangGraphì˜ í•µì‹¬ ìš”ì†Œ\n",
    "\n",
    "| ìš”ì†Œ | ì„¤ëª… | ì˜ˆì‹œ |\n",
    "|------|------|------|\n",
    "| **State** | ì›Œí¬í”Œë¡œìš° ì „ì²´ì—ì„œ ê³µìœ ë˜ëŠ” ë°ì´í„° | `{\"text\": \"...\", \"classification\": \"News\"}` |\n",
    "| **Node** | íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ | `classification_node(state)` |\n",
    "| **Edge** | ë…¸ë“œ ê°„ì˜ ì—°ê²° (íë¦„) | `A â†’ B â†’ C` |\n",
    "| **Graph** | ë…¸ë“œì™€ ì—£ì§€ë¡œ êµ¬ì„±ëœ ì „ì²´ ì›Œí¬í”Œë¡œìš° | `StateGraph(State)` |\n",
    "\n",
    "### ì£¼ìš” ë©”ì„œë“œ\n",
    "\n",
    "```python\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"node_name\", function)\n",
    "\n",
    "# ì‹œì‘ì  ì„¤ì •\n",
    "workflow.set_entry_point(\"node_name\")\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_edge(\"from_node\", \"to_node\")\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "app = workflow.compile()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "result = app.invoke(input_state)\n",
    "```\n",
    "\n",
    "### ë””ìì¸ íŒ¨í„´\n",
    "\n",
    "1. **íŒŒì´í”„ë¼ì¸ íŒ¨í„´**: A â†’ B â†’ C (ìˆœì°¨ì  ì²˜ë¦¬)\n",
    "2. **ë³‘ë ¬ ì²˜ë¦¬**: A â†’ [B, C, D] â†’ E (ë™ì‹œ ì‹¤í–‰)\n",
    "3. **ì¡°ê±´ë¶€ ë¼ìš°íŒ…**: A â†’ (ì¡°ê±´) â†’ B or C (ë¶„ê¸°)\n",
    "4. **ë£¨í”„**: A â†’ B â†’ A (ë°˜ë³µ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### 1. ë…¸ë“œ ì¶”ê°€í•˜ê¸°\n",
    "- **ê°ì • ë¶„ì„**: í…ìŠ¤íŠ¸ì˜ ê¸ì •/ë¶€ì • íŒë‹¨\n",
    "- **í‚¤ì›Œë“œ ì¶”ì¶œ**: ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "- **ë²ˆì—­**: ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­\n",
    "\n",
    "### 2. ì¡°ê±´ë¶€ ë¼ìš°íŒ… êµ¬í˜„\n",
    "```python\n",
    "# ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë‹¤ë¥¸ ê²½ë¡œë¡œ ë¼ìš°íŒ…\n",
    "def route_by_classification(state):\n",
    "    if state[\"classification\"] == \"News\":\n",
    "        return \"news_handler\"\n",
    "    else:\n",
    "        return \"general_handler\"\n",
    "```\n",
    "\n",
    "### 3. ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„\n",
    "```python\n",
    "# ê°œì²´ëª… ì¶”ì¶œê³¼ ê°ì • ë¶„ì„ì„ ë™ì‹œì— ì‹¤í–‰\n",
    "workflow.add_edge(\"classification\", \"entity_extraction\")\n",
    "workflow.add_edge(\"classification\", \"sentiment_analysis\")\n",
    "```\n",
    "\n",
    "### 4. ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€\n",
    "```python\n",
    "def safe_node(state):\n",
    "    try:\n",
    "        # ì‘ì—… ìˆ˜í–‰\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "```\n",
    "\n",
    "### 5. ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼\n",
    "```python\n",
    "# ê° ë…¸ë“œì˜ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥\n",
    "for output in app.stream(state_input):\n",
    "    print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ í•™ìŠµ íŒ\n",
    "\n",
    "### ì´í•´ë¥¼ ìœ„í•œ 3ë‹¨ê³„\n",
    "\n",
    "1. **ì½ê¸°**: ì½”ë“œë¥¼ ì²œì²œíˆ ì½ìœ¼ë©° ê° ë¶€ë¶„ì˜ ì—­í• ì„ ì´í•´í•©ë‹ˆë‹¤\n",
    "2. **ì‹¤í–‰**: ì…€ì„ í•˜ë‚˜ì”© ì‹¤í–‰í•˜ë©° ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤\n",
    "3. **ìˆ˜ì •**: ì½”ë“œë¥¼ ë³€ê²½í•´ë³´ë©° ë™ì‘ì„ ì‹¤í—˜í•©ë‹ˆë‹¤\n",
    "\n",
    "### ì‹¤ìŠµ ê³¼ì œ\n",
    "\n",
    "1. âœï¸ ìƒˆë¡œìš´ ë…¸ë“œë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš” (ì˜ˆ: ê°ì • ë¶„ì„)\n",
    "2. ğŸ”„ ë…¸ë“œì˜ ìˆœì„œë¥¼ ë³€ê²½í•´ë³´ì„¸ìš”\n",
    "3. ğŸ¨ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¡œ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”\n",
    "4. ğŸ“Š ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•´ë³´ì„¸ìš”\n",
    "\n",
    "### ë””ë²„ê¹… íŒ\n",
    "\n",
    "```python\n",
    "# ê° ë…¸ë“œì— print ë¬¸ ì¶”ê°€\n",
    "def my_node(state):\n",
    "    print(f\"í˜„ì¬ ìƒíƒœ: {state}\")\n",
    "    # ...\n",
    "    return result\n",
    "\n",
    "# ì—ëŸ¬ ë°œìƒ ì‹œ ìƒíƒœ í™•ì¸\n",
    "try:\n",
    "    result = app.invoke(state_input)\n",
    "except Exception as e:\n",
    "    print(f\"ì—ëŸ¬: {e}\")\n",
    "    print(f\"í˜„ì¬ ìƒíƒœ: {state_input}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ê²°ë¡ \n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰ ì´ì œ ì—¬ëŸ¬ë¶„ì€:\n",
    "\n",
    "âœ… LangGraphì˜ í•µì‹¬ ê°œë…ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤\n",
    "âœ… ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "âœ… ë…¸ë“œì™€ ì—£ì§€ë¡œ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "âœ… AI ëª¨ë¸ì„ ì›Œí¬í”Œë¡œìš°ì— í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "âœ… ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "### ë” í•™ìŠµí•˜ê¸°\n",
    "\n",
    "- [LangGraph ê³µì‹ ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain ë¬¸ì„œ](https://python.langchain.com/)\n",
    "- [ì˜ˆì œ ëª¨ìŒ](https://github.com/langchain-ai/langgraph/tree/main/examples)\n",
    "\n",
    "### ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´\n",
    "\n",
    "- GitHub Issues\n",
    "- Discord ì»¤ë®¤ë‹ˆí‹°\n",
    "- Stack Overflow\n",
    "\n",
    "**Happy Coding! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
